{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "* Function data_processing: return X_train, y_train, X_test, y_test (numpy arrays)\n",
    "* Train Full_NN on data and saving weights\n",
    "* Extract base_CNN with saved_weights\n",
    "* Function features_extraction(sub_NN, ...): see keras example\n",
    "* Function output_layer (features, period_snapshot): top_layers training with snapshot of weights \n",
    "* Function predictive_distribution (x_test, weight_snapshot_folder, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Input\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    "\n",
    "metrics=['accuracy']\n",
    "loss=keras.losses.categorical_crossentropy\n",
    "optimizer=keras.optimizers.Adadelta()\n",
    "SGD=keras.optimizers.SGD\n",
    "\n",
    "#from mnist import SGLD\n",
    "#import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# subset of mnist\n",
    "x_train=x_train[:6000]\n",
    "y_train=y_train[:6000]\n",
    "x_test=x_test[:1000]\n",
    "y_test=y_test[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-training phase on CNN model (From F.Chollet book > 95% accuracy with 12 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_base1 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv_base2 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv_base3 (MaxPooling2D)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_base4 (Dropout)         (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape, name='conv_base1'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', name='conv_base2'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), name='conv_base3'))\n",
    "model.add(Dropout(0.25, name='conv_base4'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss,\n",
    "              optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/3\n",
      "6000/6000 [==============================] - 20s 3ms/step - loss: 1.0510 - acc: 0.6698 - val_loss: 0.3909 - val_acc: 0.8900\n",
      "Epoch 2/3\n",
      "6000/6000 [==============================] - 21s 3ms/step - loss: 0.3317 - acc: 0.9027 - val_loss: 0.2523 - val_acc: 0.9130\n",
      "Epoch 3/3\n",
      "6000/6000 [==============================] - 26s 4ms/step - loss: 0.2164 - acc: 0.9323 - val_loss: 0.1528 - val_acc: 0.9530\n",
      "Test loss: 0.15278889936208725\n",
      "Test accuracy: 0.953\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='./output/model/mnist_cnn.h5'\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature_extraction from conv_base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_base1_input (InputLayer (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv_base1 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv_base2 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv_base3 (MaxPooling2D)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_base4 (Dropout)         (None, 12, 12, 64)        0         \n",
      "=================================================================\n",
      "Total params: 18,816\n",
      "Trainable params: 18,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# extract the base_cnn and load_weights from pre-trained network\n",
    "output_layer=model.get_layer(index=3)\n",
    "\n",
    "# avoir si obligé de recréer le modèle\n",
    "sub_model=Model(inputs=model.input, outputs=output_layer.output) # only take the conv base of the model\n",
    "print(sub_model.summary())\n",
    "\n",
    "sub_model.load_weights(model_path,by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 12, 12, 64)\n",
      "(1000, 12, 12, 64)\n",
      "(6000, 9216)\n",
      "(1000, 9216)\n"
     ]
    }
   ],
   "source": [
    "# extract features from conv_base model\n",
    "features_train=sub_model.predict(x_train)\n",
    "print(features_train.shape)\n",
    "features_test=sub_model.predict(x_test)\n",
    "print(features_test.shape)\n",
    "\n",
    "# reshaping\n",
    "import numpy as np\n",
    "features_train=np.reshape(features_train, (6000,12*12*64))\n",
    "features_test=np.reshape(features_test, (1000,12*12*64))\n",
    "print(features_train.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last_layer algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define top_layers\n",
    "top_model=Sequential()\n",
    "# add Flatten\n",
    "top_model.add(Dense(128, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 4s 606us/step - loss: 0.5713 - acc: 0.8273 - val_loss: 0.2042 - val_acc: 0.9440\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 3s 545us/step - loss: 0.2128 - acc: 0.9425 - val_loss: 0.1648 - val_acc: 0.9480\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 3s 539us/step - loss: 0.1644 - acc: 0.9542 - val_loss: 0.1230 - val_acc: 0.9600\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 3s 471us/step - loss: 0.1350 - acc: 0.9620 - val_loss: 0.1216 - val_acc: 0.9580\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 3s 424us/step - loss: 0.1111 - acc: 0.9667 - val_loss: 0.0926 - val_acc: 0.9670\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 3s 490us/step - loss: 0.0957 - acc: 0.9710 - val_loss: 0.1044 - val_acc: 0.9610\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 3s 457us/step - loss: 0.0832 - acc: 0.9758 - val_loss: 0.0889 - val_acc: 0.9690\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 3s 456us/step - loss: 0.0697 - acc: 0.9808 - val_loss: 0.0857 - val_acc: 0.9650\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 3s 468us/step - loss: 0.0708 - acc: 0.9795 - val_loss: 0.0747 - val_acc: 0.9730\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 3s 435us/step - loss: 0.0616 - acc: 0.9825 - val_loss: 0.0824 - val_acc: 0.9750\n",
      "Test loss: 0.15278889936208725\n",
      "Test accuracy: 0.953\n"
     ]
    }
   ],
   "source": [
    "# train top_layers with weights snapshots every snapshot interval\n",
    "epochs=10\n",
    "top_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "mc = keras.callbacks.ModelCheckpoint('./output/weights/weights{epoch:08d}.h5', \n",
    "                                     save_weights_only=True, period=2)\n",
    "\n",
    "top_model.fit(features_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(features_test, y_test),callbacks=[mc])\n",
    "#score = model.evaluate(features_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "top_model_path='./output/model/mnist_top_layer.h5'\n",
    "top_model.save(top_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get each h5 files with weights\n",
    "import os\n",
    "import glob\n",
    "weight_path='./output/weights/'\n",
    "os.chdir(weight_path)\n",
    "h5_files = [i for i in glob.glob('*.h5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictive_score_distribution\n",
    "n_snapshots=len(h5_files)\n",
    "predictive_scores_distribution=np.zeros(shape=(n_snapshots,1000,10))\n",
    "for index, weight_snapshot in zip(range(n_snapshots),h5_files):\n",
    "    top_model.load_weights(weight_snapshot)\n",
    "    prediction_snapshot=top_model.predict(features_test)\n",
    "    predictive_scores_distribution[index]=prediction_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1000, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictive_scores_distribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictive_class_distribution\n",
    "n_snapshots=5\n",
    "predictive_class_distribution=np.zeros(shape=(n_snapshots,1000))\n",
    "for index, weight_snapshot in enumerate(h5_files):\n",
    "    top_model.load_weights(weight_snapshot)\n",
    "    prediction_snapshot=top_model.predict_classes(features_test)\n",
    "    predictive_class_distribution[index]=prediction_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictive_class_distribution.shape # np.transpose to permute dim of the numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to create\n",
    "* function full_model: classic input + names for layers, OUTPUT=model with named layers for conv_base\n",
    "\n",
    "* function pre_training: input_data, hyperparameters, model, model_path, OUTPUT=history (+ model_saving)\n",
    "* function extract_features: INPUT=index(for sub_NN), input_data, model_path, OUTPUT=reshaped features\n",
    "* function train_lastLayer_checkpoints, INPUT=top_model, interval for checkpoint, weight_path, model_path, training_parameters (optimizer, loss, accuracy)// OUTPUT=saved_snapshots\n",
    "* function predictive_scores_distribution (model, weight_path, n_snapshot), OUTPUT: 3D-array (n_snapshots, n_samples, n_classes)\n",
    "* function predictive_class_distribution (model, weight_path, n_snapshot), OUTPUT: 2D-array (n_snapshots, n_samples)\n",
    "\n",
    "#### other alternative\n",
    "* function split_NN(full_model, index) // OUTPUT: a tuple of the base_model, top_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-distribution sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3075"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_index=[]\n",
    "for index in range(y_train.shape[0]):\n",
    "    if np.sum(y_train[index][:5])==1:\n",
    "        selected_index.append(index)\n",
    "len(selected_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_restricted=x_train[selected_index]\n",
    "x_train_restricted.shape\n",
    "y_train_restricted=y_train[selected_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "490"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ood_index=[]\n",
    "for index in range(y_test.shape[0]):\n",
    "    if np.sum(y_train[index][:5])==0:\n",
    "        ood_index.append(index)\n",
    "len(ood_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_ood=x_test[ood_index]\n",
    "y_test_ood=y_test[ood_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jupyter nbconvert --to script last_layer_AM_MNIST.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

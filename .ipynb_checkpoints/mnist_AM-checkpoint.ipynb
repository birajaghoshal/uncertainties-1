{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/46573036/extracting-last-layers-of-keras-model-as-a-submodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train a convnet model on MNISt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "###Trains a simple convnet on the MNIST dataset.\n",
    "\n",
    "###Gets to 99.25% test accuracy after 12 epochs\n",
    "###(there is still a lot of margin for parameter tuning).\n",
    "###16 seconds per epoch on a GRID K520 GPU.\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# subset of mnist\n",
    "x_train=x_train[:6000]\n",
    "y_train=y_train[:6000]\n",
    "x_test=x_test[:1000]\n",
    "y_test=y_test[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "base_CNN0 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "base_CNN1 (Conv2D)           (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "base_CNN2 (MaxPooling2D)     (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "base_CNN3 (Dropout)          (None, 14, 14, 64)        0         \n",
      "=================================================================\n",
      "Total params: 18,816\n",
      "Trainable params: 18,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_tensor=Input(shape=input_shape)\n",
    "x = Conv2D(32, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='base_CNN0')(input_tensor)\n",
    "x = Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='base_CNN1')(x)\n",
    "x = MaxPooling2D((2, 2),name='base_CNN2')(x)\n",
    "output_tensor= Dropout(0.25,name='base_CNN3')(x)\n",
    "conv_base_model = Model(input_tensor, output_tensor, name='conv_base')\n",
    "conv_base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(base_model, model_path, loss, optimizer, metrics):\n",
    "    model=Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(loss=loss,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics)\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    weights_base=base_model.get_weights()\n",
    "    model.save(model_path)\n",
    "    \n",
    "    return weights_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_base (Model)            (None, 14, 14, 64)        18816     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               1605760   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,625,866\n",
      "Trainable params: 1,625,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/3\n",
      "6000/6000 [==============================] - 23s 4ms/step - loss: 0.6541 - acc: 0.7952 - val_loss: 0.2684 - val_acc: 0.9100\n",
      "Epoch 2/3\n",
      "6000/6000 [==============================] - 20s 3ms/step - loss: 0.2483 - acc: 0.9245 - val_loss: 0.2259 - val_acc: 0.9260\n",
      "Epoch 3/3\n",
      "6000/6000 [==============================] - 20s 3ms/step - loss: 0.1798 - acc: 0.9492 - val_loss: 0.1538 - val_acc: 0.9470\n",
      "Test loss: 0.15379313784837723\n",
      "Test accuracy: 0.947\n"
     ]
    }
   ],
   "source": [
    "metrics=['accuracy']\n",
    "loss=keras.losses.categorical_crossentropy\n",
    "optimizer=keras.optimizers.Adadelta()\n",
    "model_path='./output/model/mnist_cnn.h5'\n",
    "weights_base_trained=training_model(conv_base_model,model_path, loss, optimizer, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights_base_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for the weights savings\n",
    "https://stackoverflow.com/questions/46573036/extracting-last-layers-of-keras-model-as-a-submodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "def cnn():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(28, 28, 1), name='l_01'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', name='l_02'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), name='l_03'))\n",
    "    model.add(Dropout(0.25, name='l_04'))\n",
    "    model.add(Flatten(name='l_05'))\n",
    "    model.add(Dense(128, activation='relu', name='l_06'))\n",
    "    model.add(Dropout(0.5, name='l_07'))\n",
    "    model.add(Dense(10, activation='softmax', name='l_08'))\n",
    "    return model\n",
    "\n",
    "def predictor(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(name='l_05', input_shape=(12, 12, 64)))\n",
    "    model.add(Dense(128, activation='relu', name='l_06'))\n",
    "    model.add(Dropout(0.5, name='l_07'))\n",
    "    model.add(Dense(10, activation='softmax', name='l_08'))\n",
    "    return model\n",
    "\n",
    "cnn_model = cnn()\n",
    "cnn_model.save('/tmp/cnn_model.h5')\n",
    "\n",
    "predictor_model = predictor(cnn_model.output.shape)\n",
    "predictor_model.load_weights('/tmp/cnn_model.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract features from the base CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7_input (InputLayer)  (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 12, 12, 64)        0         \n",
      "=================================================================\n",
      "Total params: 18,816\n",
      "Trainable params: 18,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "keras.engine.training.Model"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### getting the base_model from the original model\n",
    "def base_model_trained(model, index):\n",
    "    output_layer=model.get_layer(index=index)\n",
    "    sub_model=Model(inputs=model.input, outputs=output_layer.output) # only take the conv name of a model\n",
    "    print(sub_model.summary())\n",
    "    return sub_model\n",
    "sub_model=base_model(model, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features_train=sub_model.predict(x_train) ### YES CA MARCHE!!!\n",
    "# add the weigth savings for Base_conv\n",
    "# Features will be an arg of the last layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link to get the weight_savings on the base_NN: https://stackoverflow.com/questions/43702323/how-to-load-only-specific-weights-on-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features_test=sub_model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(input_data, model, weights):\n",
    "    # upload weights to base_cnn\n",
    "    model.set_weights(weights)\n",
    "    features_base_cnn=model.predict(input_data)\n",
    "    return features_base_cnn\n",
    "# should return both prediction + labels=y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple features extraction from a pre-trained model\n",
    "https://medium.com/@franky07724_57962/using-keras-pre-trained-models-for-feature-extraction-in-image-clustering-a142c6cdf5b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.6965162e-12, 1.5724373e-07, 5.2328411e-13, ..., 2.4397271e-11,\n",
       "        8.0220644e-07, 3.5690906e-07],\n",
       "       [1.0000000e+00, 4.1540935e-13, 2.4314215e-10, ..., 4.4013625e-12,\n",
       "        6.9115026e-12, 6.9341217e-12],\n",
       "       [7.9562330e-09, 3.1643969e-05, 1.1628156e-06, ..., 2.8949844e-07,\n",
       "        8.9862922e-08, 6.0002785e-06],\n",
       "       ...,\n",
       "       [3.8459133e-15, 1.4971198e-11, 3.3836449e-19, ..., 9.5785943e-18,\n",
       "        2.5468203e-09, 1.1558767e-07],\n",
       "       [1.5240547e-09, 1.1646077e-10, 3.6926048e-10, ..., 1.9749100e-15,\n",
       "        1.9257498e-11, 7.1957882e-14],\n",
       "       [2.8926337e-09, 5.1395588e-14, 2.5903706e-12, ..., 2.5598046e-13,\n",
       "        1.0000000e+00, 2.4802640e-08]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train=extract_features(x_train, model)\n",
    "features_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight savings trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save model weights every N epochs during training\n",
    "* https://stackoverflow.com/questions/51186330/save-model-weights-at-the-end-of-every-n-epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-7dfcb1357a0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m mc = keras.callbacks.ModelCheckpoint('weights{epoch:08d}.h5', \n\u001b[1;32m      3\u001b[0m                                      save_weights_only=True, period=5)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "### code to save weights\n",
    "mc = keras.callbacks.ModelCheckpoint('weights{epoch:08d}.h5', \n",
    "                                     save_weights_only=True, period=5) # save weights after every N epoch\n",
    "# period=the Interval between each epoch\n",
    "model.fit(X_train, Y_train, callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "###Trains a simple convnet on the MNIST dataset.\n",
    "\n",
    "###Gets to 99.25% test accuracy after 12 epochs\n",
    "###(there is still a lot of margin for parameter tuning).\n",
    "###16 seconds per epoch on a GRID K520 GPU.\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 184s 3ms/step - loss: 0.2733 - acc: 0.9160 - val_loss: 0.0606 - val_acc: 0.9807\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 183s 3ms/step - loss: 0.0917 - acc: 0.9728 - val_loss: 0.0392 - val_acc: 0.9863\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 169s 3ms/step - loss: 0.0683 - acc: 0.9797 - val_loss: 0.0325 - val_acc: 0.9895\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 178s 3ms/step - loss: 0.0540 - acc: 0.9831 - val_loss: 0.0307 - val_acc: 0.9887\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 202s 3ms/step - loss: 0.0473 - acc: 0.9857 - val_loss: 0.0304 - val_acc: 0.9895\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 176s 3ms/step - loss: 0.0410 - acc: 0.9878 - val_loss: 0.0297 - val_acc: 0.9906\n",
      "Test loss: 0.030052940502776165\n",
      "Test accuracy: 0.9917\n"
     ]
    }
   ],
   "source": [
    "epochs=6\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "mc = keras.callbacks.ModelCheckpoint('weights{epoch:08d}.h5', \n",
    "                                     save_weights_only=True, period=2) # save weights after every N epoch\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test),callbacks=[mc])\n",
    "#score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "#model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "* Function data_processing: return X_train, y_train, X_test, y_test (numpy arrays)\n",
    "* Function full_NN\n",
    "* Train Full_NN on data with weight_savings on sub_NN\n",
    "* Function sub_NN\n",
    "* Function features_extraction(sub_NN, ...): see keras example\n",
    "* Function output_layer (features, period_snapshot, is_training)\n",
    "> take snapshot only for\n",
    "* Function predictive_distribution (x_test, weight_snapshot_folder, model, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code for feature extraction - from F.Chollet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            # Note that since generators yield data indefinitely in a loop,\n",
    "            # we must `break` after every image has been seen once.\n",
    "            break\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = extract_features(train_dir, 2000)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 1000)\n",
    "test_features, test_labels = extract_features(test_dir, 1000)\n",
    "\n",
    "# reshaping to flatten the features\n",
    "train_features = np.reshape(train_features, (2000, 4 * 4 * 512))\n",
    "validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))\n",
    "test_features = np.reshape(test_features, (1000, 4 * 4 * 512))\n",
    "\n",
    "# fitting a top layers on the extracted features\n",
    "# This will be the output_layer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "# here the checkpoint every N epoch\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=30,\n",
    "                    batch_size=20,\n",
    "                    validation_data=(validation_features, validation_labels))\n",
    "# add callbacks in the parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
